{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZnpV6lK4i4APZjIvEIPRf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yoshiki0418/Deep_Learning/blob/main/ner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NER"
      ],
      "metadata": {
        "id": "Wq9rz8DDJ1wL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### データ準備"
      ],
      "metadata": {
        "id": "bludEmD5J3e5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**データロード**"
      ],
      "metadata": {
        "id": "QmITMBZGJ7S2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_NOiS_BJyi-",
        "outputId": "c1f67623-33d3-4bf3-a2c1-35dcd3607c87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Google Driveとの接続\n",
        "from google.colab import drive\n",
        "drive_path = '/content/drive'\n",
        "drive.mount(drive_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Deep_Learning/DL_Lecture3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbpzmozvKcu8",
        "outputId": "97f3fa4b-3dae-443b-8bb1-26e7135e4d47"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Deep_Learning/DL_Lecture3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "d5XCN22qKj9d"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(sentences_file_name=\"ner_dataset_sentences.txt\", labels_file_name=\"ner_dataset_labels.txt\"):\n",
        "    with open(sentences_file_name, \"rb\") as fp:\n",
        "        sentences = pickle.load(fp)\n",
        "    with open(labels_file_name, \"rb\") as fp:\n",
        "        labels = pickle.load(fp)\n",
        "    return sentences, labels"
      ],
      "metadata": {
        "id": "u05Ds3NAKnnP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences, labels = load_dataset()"
      ],
      "metadata": {
        "id": "1JFu1xSPLxAv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DC-AsznkL2yP",
        "outputId": "ec71218f-adb1-4c3d-b05b-8f6aed358e50"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The cat sat on the mat .',\n",
              " 'John lives in New York .',\n",
              " 'I have two dogs .',\n",
              " 'She works at Google .',\n",
              " 'The Eiffel Tower is in Paris .',\n",
              " 'He is from Spain .',\n",
              " 'I visited the Great Wall of China .',\n",
              " 'She is studying at Oxford University .',\n",
              " 'He works for the United Nations .',\n",
              " 'Berlin is the capital of Germany .']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioXaPHxhL6i7",
        "outputId": "6973cc47-77bf-49df-9149-45dffcc99188"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
              " ['B-per', 'O', 'O', 'B-geo', 'I-geo', 'O'],\n",
              " ['O', 'O', 'O', 'O', 'O'],\n",
              " ['O', 'O', 'O', 'B-org', 'O'],\n",
              " ['O', 'B-geo', 'I-geo', 'O', 'O', 'B-geo', 'O'],\n",
              " ['O', 'O', 'O', 'B-geo', 'O'],\n",
              " ['O', 'O', 'O', 'B-geo', 'I-geo', 'I-geo', 'I-geo', 'O'],\n",
              " ['O', 'O', 'O', 'O', 'B-org', 'I-org', 'O'],\n",
              " ['O', 'O', 'O', 'O', 'B-org', 'I-org', 'O'],\n",
              " ['B-geo', 'O', 'O', 'O', 'O', 'B-geo', 'O']]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ラベルエンコーディング**"
      ],
      "metadata": {
        "id": "W1FX4R7RMAam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "J5ZBIiQXMC4G"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_labels = [label for sublist in labels for label in sublist]\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(all_labels)\n",
        "encoded_labels = [label_encoder.transform(label) for label in labels]"
      ],
      "metadata": {
        "id": "tnI_0LOOMJJI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_labels[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzh9y251Ms4e",
        "outputId": "3d217c2c-8c19-47b6-d953-276259c6be8e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([6, 6, 6, 6, 6, 6, 6]),\n",
              " array([2, 6, 6, 0, 3, 6]),\n",
              " array([6, 6, 6, 6, 6])]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**辞書作成とエンコード**"
      ],
      "metadata": {
        "id": "1jzKldi6NbCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 辞書の初期化\n",
        "word2idx = {\"<PAD>\": 0}"
      ],
      "metadata": {
        "id": "RkX_rfWTNepF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_sentences = []\n",
        "for sentence in sentences:\n",
        "    encoded_sentence = [word2idx.setdefault(word, len(word2idx)) for word in sentence.split()]\n",
        "    encoded_sentences.append(encoded_sentence)"
      ],
      "metadata": {
        "id": "LtdoFvw5QboI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_sentences[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wjup--JoRyke",
        "outputId": "623e2735-f5f2-483a-c74d-86f26328cfec"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 7], [13, 14, 15, 16, 7]]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**学習データと検証データ分割**"
      ],
      "metadata": {
        "id": "fDbKO22cR7G3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "-YjmQMiER9_w"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val, train_sentences, val_sentences = train_test_split(encoded_sentences, encoded_labels, sentences, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "k2H244MlSFxS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJ0uYnAUSUea",
        "outputId": "02da1998-2973-4432-84bf-da260ce193f8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[94, 51, 95, 96, 97, 98, 7], [17, 18, 19, 87, 7], [1, 50, 51, 52, 10, 53, 7]]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oo4IpgW9SXcu",
        "outputId": "ab938be7-ba54-42fe-b7c0-2ed70e40b9c7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['They are visiting London this summer .',\n",
              " 'She works at Facebook .',\n",
              " 'The Pyramids are located in Egypt .']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**padding**"
      ],
      "metadata": {
        "id": "pjvEWckRXOPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "seq1 = torch.tensor([1, 2, 3])\n",
        "seq2 = torch.tensor([4, 5])\n",
        "seq3 = torch.tensor([6, 7, 8, 9])\n",
        "\n",
        "sequences = [seq1, seq2, seq3]\n",
        "padded_senquences = pad_sequence(sequences, batch_first=True)\n",
        "print(padded_senquences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9EbvWqsXP4c",
        "outputId": "bbf7843c-cdbf-4e5e-bc90-29214efbd4d0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3, 0],\n",
            "        [4, 5, 0, 0],\n",
            "        [6, 7, 8, 9]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pad_sequence([torch.tensor(x) for x in X_train], batch_first=True)\n",
        "X_val = pad_sequence([torch.tensor(x) for x in X_val], batch_first=True)\n",
        "y_train = pad_sequence([torch.tensor(y) for y in y_train], batch_first=True)\n",
        "y_val = pad_sequence([torch.tensor(y) for y in y_val], batch_first=True)"
      ],
      "metadata": {
        "id": "pq1xajBmYeK3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sj986M_hYg-b",
        "outputId": "4f78719e-6fa9-433d-9ce7-56e941b6823b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 94,  51,  95,  96,  97,  98,   7,   0,   0,   0],\n",
              "        [ 17,  18,  19,  87,   7,   0,   0,   0,   0,   0],\n",
              "        [  1,  50,  51,  52,  10,  53,   7,   0,   0,   0],\n",
              "        [  1,   2,   3,   4,   5,   6,   7,   0,   0,   0],\n",
              "        [  1,  21,  22,  23,  10,  24,   7,   0,   0,   0],\n",
              "        [ 25,  23,  60,  61,  24,  36,  62,  63,   7,   0],\n",
              "        [ 25,  23,  26,  27,   7,   0,   0,   0,   0,   0],\n",
              "        [  1,  54,  55,  23,  10,  56,   7,   0,   0,   0],\n",
              "        [  1,  47,  23,   5,  48,  49,  10,   5,  46,   7],\n",
              "        [  1,  82,  22,  31,  83,  23,  10,  84,   7,   0],\n",
              "        [  8,   9,  10,  11,  12,   7,   0,   0,   0,   0],\n",
              "        [ 13,  14,  15,  16,   7,   0,   0,   0,   0,   0],\n",
              "        [  1,  88,  89,  23,  10,  90,   7,   0,   0,   0],\n",
              "        [ 17,  18,  19,  20,   7,   0,   0,   0,   0,   0],\n",
              "        [ 78,  79,  80,  62,  81,   7,   0,   0,   0,   0],\n",
              "        [ 17,  23,  33,  19,   5,  35,  31,  91,   7,   0],\n",
              "        [ 25,  67,  19,  68,   7,   0,   0,   0,   0,   0],\n",
              "        [ 25,  18,  36,   5,  99, 100, 101,   7,   0,   0],\n",
              "        [ 73,  23,  74,  36,  75,  76,  77,   7,   0,   0],\n",
              "        [ 17,  23,  33,  19,  34,  35,   7,   0,   0,   0],\n",
              "        [ 42,  43,  23,   5,  44,  45,  10,   5,  46,   7],\n",
              "        [ 13,  57,  19,  58,   7,   0,   0,   0,   0,   0],\n",
              "        [ 69,  70,  23,  62,  71,  72,   7,   0,   0,   0],\n",
              "        [ 13,  28,   5,  29,  30,  31,  32,   7,   0,   0]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVzWVbfxYmrN",
        "outputId": "ba937c3f-4b3c-4685-ff1f-acd74e24f7ea"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([24, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NERのモデル"
      ],
      "metadata": {
        "id": "7xQDiAjgfmkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn"
      ],
      "metadata": {
        "id": "fUY_6qBJfoeX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, num_layers=1, rnn_type=\"LSTM\", bidirectional=False):\n",
        "        super().__init__()\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        # embedding layer追加 (vocab_size x embedding_size)\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        input_size = embedding_dim\n",
        "\n",
        "        if rnn_type == 'RNN':\n",
        "            self.rnn = nn.RNN(input_size, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n",
        "        elif rnn_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n",
        "        elif rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n",
        "        else:\n",
        "            raise ValueError('Unsupported RNN type. Choose from [\"LSTM\", \"RNN\", \"GRU\", \"UGRNN\"]')\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size*self.num_directions, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output_seq, _ = self.rnn(x)\n",
        "\n",
        "        # many to many の場合は，\n",
        "        # output_seq: [batch_size, seq_len, hidden_size*num_directions]\n",
        "        out = self.fc(output_seq)\n",
        "        return out"
      ],
      "metadata": {
        "id": "UpE6Ga5agTaB"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BiRNNのテスト\n",
        "# input_size = 10\n",
        "vocab_size = 300 # モデルが扱う語彙の総数（ユニークなトークンの数 + 特殊トークン）\n",
        "embedding_dim = 50\n",
        "hidden_size = 3\n",
        "batch_size = 24\n",
        "seq_len = 10. # paddingを含めた文章の長さ\n",
        "output_size = 3\n",
        "\n",
        "model = Model(vocab_size, embedding_dim, hidden_size, output_size, bidirectional=True, )\n",
        "out = model(X_train)"
      ],
      "metadata": {
        "id": "kxKo9pvBh6Fj"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdtGz1IEiZgK",
        "outputId": "32c73924-884d-44f0-815c-00a01ce280f9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1755, -0.1338,  0.0672],\n",
              "        [-0.0896, -0.4480,  0.1995],\n",
              "        [-0.0361, -0.3296,  0.1930],\n",
              "        [ 0.0121, -0.4118,  0.2468],\n",
              "        [ 0.0512, -0.4223,  0.2425],\n",
              "        [ 0.1781, -0.3577,  0.2110],\n",
              "        [ 0.0789, -0.5249,  0.4050],\n",
              "        [ 0.3242, -0.5079,  0.3783],\n",
              "        [ 0.3157, -0.4999,  0.3796],\n",
              "        [ 0.2968, -0.4966,  0.3711]], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### モデルの学習"
      ],
      "metadata": {
        "id": "09HxsBIskksf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "vocab_size = len(word2idx)\n",
        "num_classes = len(label_encoder.classes_)\n",
        "embedding_dim = 50\n",
        "hidden_size = 40\n",
        "output_size = num_classes\n",
        "batch_size = 3\n",
        "learning_rate = 0.003\n",
        "num_epochs = 40\n",
        "\n",
        "# モデル作成\n",
        "model = Model(vocab_size, embedding_dim, hidden_size, output_size, num_layers=1, rnn_type='LSTM', bidirectional=True)\n",
        "\n",
        "# Data Loader作成\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 損失関数とOptimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "# 学習ループ\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for i, (sentences, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(sentences)\n",
        "        loss = criterion(outputs.view(-1, num_classes), labels.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # 検証データを使用して検証エラーを計算\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    total_samples = 0\n",
        "    total_correct = 0\n",
        "    for sentences, labels in val_loader:\n",
        "\n",
        "        outputs = model(sentences)\n",
        "        # loss計算\n",
        "        loss = criterion(outputs.view(-1, num_classes), labels.view(-1))\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        # accuracy計算\n",
        "        _, predicted = torch.max(outputs, dim=-1)\n",
        "        # Padding部分を除外するマスクを作成\n",
        "        non_pad_elements = labels != 0\n",
        "        # Paddingを除いた予測結果とラベルを比較し、正解数をカウント\n",
        "        total_correct += (predicted[non_pad_elements] == labels[non_pad_elements]).sum().item()\n",
        "        # 正確なサンプル数（Paddingを除いた要素数）をカウント\n",
        "        total_samples += non_pad_elements.sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_accuracy = total_correct / total_samples\n",
        "    print(f\"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEOWtHP6kl4A",
        "outputId": "68d118d7-e468-4f55-82da-21f32a95cd39"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40, Average Training Loss: 1.7674\n",
            "Val Loss: 1.3924, Val Accuracy: 0.8485\n",
            "Epoch 2/40, Average Training Loss: 1.0996\n",
            "Val Loss: 0.6816, Val Accuracy: 0.8485\n",
            "Epoch 3/40, Average Training Loss: 0.6477\n",
            "Val Loss: 0.6044, Val Accuracy: 0.8485\n",
            "Epoch 4/40, Average Training Loss: 0.5886\n",
            "Val Loss: 0.5278, Val Accuracy: 0.8485\n",
            "Epoch 5/40, Average Training Loss: 0.4375\n",
            "Val Loss: 0.4967, Val Accuracy: 0.8485\n",
            "Epoch 6/40, Average Training Loss: 0.3448\n",
            "Val Loss: 0.4987, Val Accuracy: 0.8485\n",
            "Epoch 7/40, Average Training Loss: 0.2558\n",
            "Val Loss: 0.5331, Val Accuracy: 0.8485\n",
            "Epoch 8/40, Average Training Loss: 0.1877\n",
            "Val Loss: 0.4645, Val Accuracy: 0.8788\n",
            "Epoch 9/40, Average Training Loss: 0.1430\n",
            "Val Loss: 0.4722, Val Accuracy: 0.8788\n",
            "Epoch 10/40, Average Training Loss: 0.1061\n",
            "Val Loss: 0.5197, Val Accuracy: 0.8788\n",
            "Epoch 11/40, Average Training Loss: 0.0812\n",
            "Val Loss: 0.5475, Val Accuracy: 0.8788\n",
            "Epoch 12/40, Average Training Loss: 0.0610\n",
            "Val Loss: 0.5741, Val Accuracy: 0.8788\n",
            "Epoch 13/40, Average Training Loss: 0.0465\n",
            "Val Loss: 0.5688, Val Accuracy: 0.8788\n",
            "Epoch 14/40, Average Training Loss: 0.0393\n",
            "Val Loss: 0.5736, Val Accuracy: 0.8788\n",
            "Epoch 15/40, Average Training Loss: 0.0318\n",
            "Val Loss: 0.5782, Val Accuracy: 0.8788\n",
            "Epoch 16/40, Average Training Loss: 0.0264\n",
            "Val Loss: 0.5855, Val Accuracy: 0.8788\n",
            "Epoch 17/40, Average Training Loss: 0.0225\n",
            "Val Loss: 0.5910, Val Accuracy: 0.8788\n",
            "Epoch 18/40, Average Training Loss: 0.0173\n",
            "Val Loss: 0.5942, Val Accuracy: 0.8788\n",
            "Epoch 19/40, Average Training Loss: 0.0153\n",
            "Val Loss: 0.5971, Val Accuracy: 0.8788\n",
            "Epoch 20/40, Average Training Loss: 0.0134\n",
            "Val Loss: 0.5976, Val Accuracy: 0.8788\n",
            "Epoch 21/40, Average Training Loss: 0.0118\n",
            "Val Loss: 0.6046, Val Accuracy: 0.8788\n",
            "Epoch 22/40, Average Training Loss: 0.0101\n",
            "Val Loss: 0.6078, Val Accuracy: 0.8788\n",
            "Epoch 23/40, Average Training Loss: 0.0093\n",
            "Val Loss: 0.6114, Val Accuracy: 0.8788\n",
            "Epoch 24/40, Average Training Loss: 0.0084\n",
            "Val Loss: 0.6144, Val Accuracy: 0.9091\n",
            "Epoch 25/40, Average Training Loss: 0.0077\n",
            "Val Loss: 0.6183, Val Accuracy: 0.9091\n",
            "Epoch 26/40, Average Training Loss: 0.0067\n",
            "Val Loss: 0.6178, Val Accuracy: 0.9091\n",
            "Epoch 27/40, Average Training Loss: 0.0061\n",
            "Val Loss: 0.6237, Val Accuracy: 0.9091\n",
            "Epoch 28/40, Average Training Loss: 0.0055\n",
            "Val Loss: 0.6254, Val Accuracy: 0.9091\n",
            "Epoch 29/40, Average Training Loss: 0.0054\n",
            "Val Loss: 0.6294, Val Accuracy: 0.9091\n",
            "Epoch 30/40, Average Training Loss: 0.0049\n",
            "Val Loss: 0.6310, Val Accuracy: 0.9091\n",
            "Epoch 31/40, Average Training Loss: 0.0047\n",
            "Val Loss: 0.6330, Val Accuracy: 0.9091\n",
            "Epoch 32/40, Average Training Loss: 0.0042\n",
            "Val Loss: 0.6368, Val Accuracy: 0.9091\n",
            "Epoch 33/40, Average Training Loss: 0.0040\n",
            "Val Loss: 0.6403, Val Accuracy: 0.9091\n",
            "Epoch 34/40, Average Training Loss: 0.0037\n",
            "Val Loss: 0.6430, Val Accuracy: 0.9091\n",
            "Epoch 35/40, Average Training Loss: 0.0035\n",
            "Val Loss: 0.6437, Val Accuracy: 0.9091\n",
            "Epoch 36/40, Average Training Loss: 0.0034\n",
            "Val Loss: 0.6480, Val Accuracy: 0.9091\n",
            "Epoch 37/40, Average Training Loss: 0.0032\n",
            "Val Loss: 0.6480, Val Accuracy: 0.9091\n",
            "Epoch 38/40, Average Training Loss: 0.0029\n",
            "Val Loss: 0.6507, Val Accuracy: 0.9091\n",
            "Epoch 39/40, Average Training Loss: 0.0029\n",
            "Val Loss: 0.6525, Val Accuracy: 0.9091\n",
            "Epoch 40/40, Average Training Loss: 0.0026\n",
            "Val Loss: 0.6544, Val Accuracy: 0.9091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 検証データの出力確認"
      ],
      "metadata": {
        "id": "J6UIRXAmmeEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "val_outputs = model(X_val)"
      ],
      "metadata": {
        "id": "B6IFpBdJmf-a"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, predicted_labels = torch.max(val_outputs, dim=-1)"
      ],
      "metadata": {
        "id": "L87VB7e5mihd"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence, label in zip(val_sentences, predicted_labels):\n",
        "    words = sentence.split()\n",
        "    decoded_labels = label_encoder.inverse_transform(label[:len(words)])\n",
        "    print(f'original sentence: {sentence}')\n",
        "    print(f'predicted labels: {decoded_labels}')\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhn21rZ6mw_X",
        "outputId": "bdf9ff12-b504-49ea-b12c-e6e1764f8e4a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original sentence: He lives in Los Angeles .\n",
            "predicted labels: ['O' 'O' 'O' 'O' 'O' 'O']\n",
            "\n",
            "original sentence: She is from Australia .\n",
            "predicted labels: ['O' 'O' 'O' 'O' 'O']\n",
            "\n",
            "original sentence: The Great Barrier Reef is in Australia .\n",
            "predicted labels: ['O' 'B-per' 'I-geo' 'I-geo' 'O' 'O' 'O' 'O']\n",
            "\n",
            "original sentence: The Amazon is the largest rainforest .\n",
            "predicted labels: ['O' 'O' 'O' 'O' 'O' 'O' 'O']\n",
            "\n",
            "original sentence: He works for the United Nations .\n",
            "predicted labels: ['O' 'O' 'O' 'O' 'O' 'O' 'O']\n",
            "\n",
            "original sentence: Berlin is the capital of Germany .\n",
            "predicted labels: ['O' 'O' 'O' 'O' 'O' 'O' 'O']\n",
            "\n"
          ]
        }
      ]
    }
  ]
}